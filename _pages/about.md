---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My PhD research interests lie in action recognition under complex environments, video content analysis, and computing science. My ultimate aim is to use machine intelligence to help understand human action, facilitating daily life and health. I am currently delving into spike vision.


# 🔥 Action-related News
- *2024.12*: &nbsp;🎉🎉 One paper was accepted by Journal of Image and Graphics, 中国图象图形学报. 
- *2024.08*: &nbsp;🎉🎉 One paper was accepted by PR.
- *2024.07*: &nbsp;🎉🎉 One paper was accepted by ACM MM 2024 International Workshop on Human-centric Multimedia Analysis.
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024.
- *2024.03*: &nbsp;🎉🎉 One co-authored paper was accepted by CVPR 2024.
- *2023.10*: &nbsp;🎉🎉 One co-authored paper was accepted by ICIP.
- *2023.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2023.
- *2023.06*: &nbsp;🎉🎉 Two co-authored papers were accepted by ICASSP 2023.
- *2023.05*: &nbsp;🎉🎉 One paper was accepted by TIP.
- *2022.06*: &nbsp;🎉🎉 One co-authored paper was accepted by ICASSP 2022.
- *2021.11*: &nbsp;🎉🎉 Two co-authored papers were accepted by PRICAI 2021.
  
# 🔥 Spike-related News
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024.
- *2024.04*: &nbsp;🎉🎉 One co-authored paper was accepted by TCDS.
- 
# 🔥 Other News
- *2024.10*: &nbsp;🎉🎉 One co-authored paper was accepted by AAAI2025.
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024 Workshop on Multi-modal Misinformation Governance in the Era of Foundation Models.
- *2023.11*：&nbsp;🎉🎉 One co-authored paper was accepted by PR.
- *2023.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2023.
- *2023.06*: &nbsp;🎉🎉 Two co-authored papers were accepted by ICASSP 2023.
- *2022.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ICME 2022.
- *2022.06*: &nbsp;🎉🎉 One co-authored paper was accepted by TIP!
- *2021.11*: &nbsp;🎉🎉 One co-authored paper was accepted by MMM!
- *2021.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2021.
- *2020.06*: &nbsp;🎉🎉 One co-authored paper was accepted by ICMR 2020.

# 📝 Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PR</div><img src='_pages/DSMF0422_1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic and static mutual fitting for action recognition](https://www.sciencedirect.com/science/article/abs/pii/S003132032400699X)

**Wenxuan Liu**, Xuemei Jia, Xian Zhong, Kui Jiang, Xiaohan Yu, Mang Ye

[**PDF**](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320324X00096/1-s2.0-S003132032400699X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIF1U5uXWS%2FOm3JXuwaGPChBKbU7e4ToGrr%2BHkOQ%2F2zpTAiEAg%2BTiI1MiNeJVaCC0xzWRWbJwbJqlOMcO%2FcW6d9KQQl4quwUIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDLoAVzem1PRBfuFNVyqPBfE9n8dKH4XrFE%2Bb8IELBSNpTYWrYxiJaRvjVK4kBjriN1udR9NQW2bUlqWY0x%2BhO34XzPj6cf3FAYwJ4QbJB3Fex62cBpH530Xanc1AzOoalrD7CZEpGdEpBkpuLTYyuH%2FZJbpD%2F%2FcTrrnejkeLTV7ryzNRKpAuL%2FQnE%2FsuXYynv4cesny66Gq7j9va2z5GHY6CG6P4EKMeDVlZ1eAkhzzhaZZLFKaqnDpbGW3LQAUEGv0xzPm0bMiF0MMK1dblialTey7C69MwBG9SyTnGAACLnnd8fp2kw%2FuFwgkilknD3Om%2FMejWlTvoYstkQkVY4cxKbtsu48jpxwBC1TbncoVzaiPsvCt7xu0xwCJVUshXoJKjaSz5fQwT1Wjys%2FoKhWlNwq0JdqKoWEXTMkge0UBLrZLu8MD%2B0tJEcmakQhUhF%2FJqssIOFxKSMuvndT5PIpxXYJslRJGdloP%2BoV8mjINMMcXqiAqBWWnWJokrImfsyoxI7VlDqIqPwXdxpN8RsrlxODa5uahY3wTJkt4WP6i4tUMcYEvJ7Z8AYBsSH62AfM3PLkHJHXfC54iu4g5AG1U6C4YnJZ6bblK%2BYT5%2Bkv9%2BD%2FrkbucoZWvey9M22OXoGZPG0DiS2%2Fw7XpA%2BfOG4GIS8yZiQLLOtXWnD2prYrgfDGqpEmwtknwEFkKtAKOCetWkbV6OtqFyFqnWuaLImoFXkw63WOdfgyJNJPFDzVSmnIi%2BtYBqIEs8LQpv8MsXdIRiK%2FXmF0Yu%2FRXdTuRetcoU9x66RWOiS1dKZfVDqRZBYNzi6TIx2Fx0QAgKQ7yrNdQn%2BAhtLeEe1IY%2FjpNsQimJ311z4vppULbTMA0KeeG7C%2BAJAU8LSFw%2FgugeH9ugwj%2FLfugY6sQF2rXMdHBIRYBe1qoeeGgzqoXqoAHEFYgUbW%2F87qJlDBEl4yr4MeQgZlC%2Feicd3XIUHYgBMM87Hu6DliN%2FXGkPZWqqD8B9TCHE9h6r8EGYhgmeLinJp2hAeNGnUNr0IqyADn4x5nNbXu3Tw%2BuwlWVhzdMSDC%2FuqW1vT8LkuTdy9H1avMO%2BM6Co%2BJty5NKropLPikM4e3GFXQlIt4Il266bCEYNoEmW3itXtk8NpOaAL3E4%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241210T093545Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUAQ5FZ5J%2F20241210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b841bd330c2f20490b381c1c57b56212329f65393ecaa669dc67d90781732bf2&hash=ecb7ee46ad52995363054f5728a92424266d5c804c4f8430c42cccd563f95b59&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S003132032400699X&tid=spdf-aae64fbc-1e79-443c-97f6-56625a275d31&sid=c15ee3ac8f5c964ea968d122cd8d6c6150a4gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=190b5f0a070357035c5656&rr=8efc3f267f40ddc0&cc=cn) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose Dynamic Temporal-aware Erasing (DTE) to excavate the fine-grained representation based on actor trajectory without destroying temporal information. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PR</div><img src='_pages/0809_1.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Pixel-REfocused Navigated Tri-margin for Semi-supervised Action Detection](https://dl.acm.org/doi/abs/10.1145/3688865.3689478)

**Wenxuan Liu**, Shilei Zhao, Xiyu Han, Aoyu Yi, Kui Jiang, Zheng Wang, Xian Zhong

[**PDF**](https://dl.acm.org/doi/pdf/10.1145/3688865.3689478) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose Dynamic Temporal-aware Erasing (DTE) to excavate the fine-grained representation based on actor trajectory without destroying temporal information. 
</div>
</div>


- [Pioneering Explainable Video Fact-Checking with a New Dataset and Multi-role Multimodal Model Approach](https://github.com), Kaipeng Niu, Danni xu, Bingjian Yang, **Wenxuan Liu**, Zheng Wang, **AAAI 2025**
- [Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks](https://github.com/hsw0929/HSD), Xian Zhong, Shengwang Hu, **Wenxuan Liu***, Wenxin Huang, Jianhao Ding, Zhaofei Yu, Tiejun Huang, **ACM MM 2024**
- [Predicting the Unseen: A Novel Dataset for Hidden Intention Localization in Pre-abnormal Analysis](https://github.com/Zzz99999/Hidden_Abnormal_Intention), Zehao Qi, Ruixu Zhang, Xinyi Hu, **Wenxuan Liu**, Zheng Wang, **ACM MM 2024**

# 🎖 Honors and Awards
- *2024.10* ACM MM 2024 the 5-th International Workshop on Human-centric Multimedia Analysis, Best Student Paper Runner-Up.
- *2024.07* Outstanding Graduate Student of Wuhan University of Technology.
- *2023.07* First Prize in the ICME ’23 Grand Challenge “Seeing Through the Rain (STRAIN): Vision Task Challenges in Real-world Rain Scenes”, Track 3.
- *2022.08* Third Prize in the 11th China Software Cup Student Software Design Competition

# 📖 Educations
- *2024.06 - now*, Postdoctor, Peking University, China.
- *2019.09 - 2024.06*, Ph D, Wuhan University of Technology, China.
- *2017.09 - 2019.06*, Master, Wuhan University of Technology, China.
- *2012.09 - 2016.06*, Bachelor, Xi’an Polytechnic of University, China. 

